{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab 사용 시 주석 풀고 mount_path 설정 후 실행\n",
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab 사용 시 주석 풀고 실행\n",
    "# !pip install transformers\n",
    "# vscode\n",
    "# %pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab 사용 시 주석 풀고 실행\n",
    "# !pip install seaborn\n",
    "# vscode\n",
    "# %pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _init import *\n",
    "\n",
    "from commons import file_util, string_util\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from tensorflow.python.keras.optimizer_v1 import Adam\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from transformers import TFBertForSequenceClassification, TFBertModel, AutoTokenizer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertModel, BertConfig\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix, classification_report, matthews_corrcoef, cohen_kappa_score, log_loss\n",
    "\n",
    "# file_dir variable\n",
    "WORK_DIR = \"../../../\"\n",
    "IN_DIR = WORK_DIR + \"resources/keyword_extract/\"\n",
    "OUT_DIR = WORK_DIR + \"resources/keyword_extract_model/bert_model.h5\"\n",
    "OUT_GRAPH_PATH = WORK_DIR + \"resources/keyword_extract_model/bert_accuracy.png\"\n",
    "OUT_REPORTFILE_PATH = WORK_DIR + \"resources/keyword_extract_model/cl_report_bert_model.csv\"\n",
    "OUT_CFMATRIX_PATH = WORK_DIR + \"resources/keyword_extract_model/matrix/cf_matrix_bert_model.png\"\n",
    "OUT_METRIXFILE_PATH = WORK_DIR + \"resources/keyword_extract_model/matrix/bert_model_metric.csv\"\n",
    "ENCODING = \"UTF-8\"\n",
    "DELIM = \"\\t\"\n",
    "DECIMAL_POINT = 2\n",
    "\n",
    "# Hyperparameter variable\n",
    "BATCH_SIZE = 32\n",
    "MAX_SEQ_LEN = 128\n",
    "DROPOUT_RATE = 0.2\n",
    "DENCE_UNIT = 2\n",
    "LEARNING_RATE = 2e-5\n",
    "PATIENCE = 5\n",
    "VERBOSE = 1\n",
    "EPOCHS = 25\n",
    "\n",
    "# bert-model\n",
    "# BERT_MODEL_NAME = \"bert-base-uncased\"\n",
    "BERT_MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "# BERT_MODEL_NAME = \"klue/bert-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# GPU divice name checker\n",
    "if device_name == \"/device:GPU:0\" :\n",
    "    print(\"Found GPU at : {}\".format(device_name))\n",
    "else :\n",
    "    print(\"GPU device not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device setting\n",
    "if torch.cuda.is_available() :\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print(\"There are %d GPU(s) available.\" % torch.cuda.device_count())\n",
    "    print(\"We will use the GPU : \", torch.cuda.get_device_name(0))\n",
    "else :\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU available, using the CPU instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer 설정\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "# tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME, do_lower_case=False)\n",
    "config = BertConfig.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "# model 설정 사용할 모델만 활성화\n",
    "# model = BertModel.from_pretrained(BERT_MODEL_NAME)\n",
    "model = TFBertModel.from_pretrained(BERT_MODEL_NAME, config=config)\n",
    "# model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vs code 사용 시 데이터 불러오기\n",
    "# data -> list로 넘길 때 사용\n",
    "def load(in_file_path: str, encoding: str, out_list: list) :\n",
    "\tfile_paths = file_util.get_file_paths(in_file_path, True)\n",
    "\n",
    "\tfor file_path in file_paths :\n",
    "\t\tin_file = file_util.open_file(file_path, encoding, \"r\")\n",
    "\n",
    "\t\twhile True :\n",
    "\t\t\tline = in_file.readline()\n",
    "\n",
    "\t\t\tif not line :\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tline = file_util.preprocess(line)\n",
    "\t\t\tif string_util.is_empty(line, True) :\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tout_list.append(line)\n",
    "\tin_file.close()\n",
    "\n",
    "# 이거 활성화 시켜서 사용하면 됨.\n",
    "# raw_data = []\n",
    "# load(IN_DIR, ENCODING, raw_data)\n",
    "# raw_data = [line.strip().split(DELIM) for line in raw_data]\n",
    "\n",
    "# raw_df = pd.DataFrame(raw_data, columns=[\"train\", \"label\"])\n",
    "# print(raw_df.shape)\n",
    "# raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name 추출\n",
    "file_paths = file_util.get_file_paths(IN_DIR, True)\n",
    "\n",
    "for file_path in file_paths :\n",
    "\tfile_name = file_util.get_file_name(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_df 생성\n",
    "raw_df = pd.read_csv(IN_DIR + file_name, sep=DELIM, names=[\"train\", \"label\"])\n",
    "print(raw_df.shape)\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set, test_set 분할\n",
    "# train 8.1, validation 0.9, test 1.0\n",
    "print(\"여기서부터 문맥에 맞게 설정\")\n",
    "print(f\"train - test : {int(raw_df.shape[0] * 0.9)}\")\n",
    "DATA_RATE = 11277\n",
    "\n",
    "train_df = raw_df[:DATA_RATE]\n",
    "test_df = raw_df[DATA_RATE:]\n",
    "train_df.tail(), test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = train_df.train\n",
    "labels = train_df.label.values\n",
    "sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids, attencion 설정\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sent in tqdm(sentences) :\n",
    "    b_input_texts = tokenizer.encode_plus(\n",
    "        sent,                               # 입력된 문장\n",
    "        add_special_tokens = True,          # \n",
    "        max_length = MAX_SEQ_LEN,           # 입력된 문장의 최대 길이\n",
    "        pad_to_max_length = True,           # \n",
    "        return_attention_mask = True        # \n",
    "\t)\n",
    "    input_ids.append(b_input_texts[\"input_ids\"])\n",
    "    attention_masks.append(b_input_texts[\"attention_mask\"])\n",
    "\n",
    "input_ids = np.asarray(input_ids)\n",
    "attention_masks = np.array(attention_masks)\n",
    "target = np.array(pd.get_dummies(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set, validation_set 분할\n",
    "X_train, X_label, y_train, y_label, train_mask, label_mask = train_test_split(\n",
    "    input_ids,\n",
    "    target,\n",
    "    attention_masks,\n",
    "    test_size=0.1\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_) :\n",
    "\tinput_ids = tf.keras.Input(shape=(MAX_SEQ_LEN,), dtype=\"int64\")\n",
    "\tattention_masks = tf.keras.Input(shape=(MAX_SEQ_LEN, ), dtype=\"int64\")\n",
    "\n",
    "\toutput = model_(input_ids, attention_masks)\n",
    "\toutput = output[0]\n",
    "\toutput = output[:, 0, :]\n",
    "\n",
    "\toutput = tf.keras.layers.Dense(BATCH_SIZE, activation=\"relu\")(output)\n",
    "\toutput = tf.keras.layers.Dropout(DROPOUT_RATE)(output)\n",
    "\toutput = tf.keras.layers.Dense(DENCE_UNIT, activation=\"softmax\")(output)\n",
    "\tmodel = tf.keras.models.Model(inputs=[input_ids, attention_masks], outputs=output)\n",
    "\n",
    "\tfor layer in model.layers[:3] :\n",
    "\t\tlayer.trainable = False\n",
    "\n",
    "\treturn model\n",
    "\n",
    "model = create_model(model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_compile(model) :\n",
    "\tadam = Adam(learning_rate=LEARNING_RATE)\n",
    "\tmodel.compile(loss=\"categorical_crossentropy\", optimizer=adam, metrics=[\"accuracy\"])\n",
    "    \n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor = \"val_accuracy\",\n",
    "    min_delta = 1e-3,\n",
    "    patience = PATIENCE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 후 저장\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath = OUT_DIR,\n",
    "    monitor = \"val_accuracy\",\n",
    "    mode = \"max\",\n",
    "    save_best_only = True,\n",
    "    verbose = VERBOSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "history = model.fit(\n",
    "    [X_train, train_mask],\n",
    "    y_train,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    epochs = EPOCHS,\n",
    "    validation_data = ([X_label, label_mask], y_label),\n",
    "    callbacks = [early_stopping, model_checkpoint],\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도 그래프 생성 및 저장\n",
    "train_accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy = history.history[\"val_accuracy\"]\n",
    "\n",
    "epochs = range(1, len(train_accuracy) + 1)\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.title(\"Bert accuracy\")\n",
    "plt.plot(epochs, train_accuracy, \"b\", label=\"train_acc\")\n",
    "plt.plot(epochs, val_accuracy, \"r\", label=\"val_acc\")\n",
    "plt.grid()\n",
    "plt.ylim(0.55, 0.70)\n",
    "plt.legend()\n",
    "plt.savefig(OUT_GRAPH_PATH)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tf.keras.models.load_model(\n",
    "    OUT_DIR,\n",
    "    custom_objects = {\n",
    "        \"TFBertModel\" : TFBertModel\n",
    "\t})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_test_res = best_model.evaluate([X_label, label_mask], y_label)\n",
    "best_test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_value = best_model.predict([X_label, label_mask])\n",
    "predicted_label = np.argmax(predicted_value, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y_test = []\n",
    "\n",
    "for value in y_label :\n",
    "\tif value[0] == 0 :\n",
    "\t\tnew_y_test.append(1)\n",
    "\telse :\n",
    "\t\tnew_y_test.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_report = classification_report(new_y_test, predicted_label, output_dict = True)\n",
    "cl_report_df = pd.DataFrame(cl_report).transpose()\n",
    "cl_report_df = cl_report_df.round(3)\n",
    "cl_report_df.to_csv(OUT_REPORTFILE_PATH)\n",
    "print(cl_report_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion_matrix 이미지 생성 및 저장\n",
    "cf_matrix = confusion_matrix(new_y_test, predicted_label)\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "sns.heatmap(cf_matrix, annot=True, fmt=\"d\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.savefig(OUT_CFMATRIX_PATH)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 지표 저장하기 위한 변수들\n",
    "accuracy_score_v = round(accuracy_score(new_y_test, predicted_label), DECIMAL_POINT)\n",
    "precision_score_v = round(precision_score(new_y_test, predicted_label, average=\"weighted\"), DECIMAL_POINT)\n",
    "recall_score_v = round(recall_score(new_y_test, predicted_label, average=\"weighted\"), DECIMAL_POINT)\n",
    "f1_score_v = round(f1_score(new_y_test, predicted_label, average=\"weighted\"), DECIMAL_POINT)\n",
    "roc_auc_score_v = round(roc_auc_score(new_y_test, predicted_label, average=\"weighted\"), DECIMAL_POINT)\n",
    "cohen_kappa_score_v = round(cohen_kappa_score(new_y_test, predicted_label), DECIMAL_POINT)\n",
    "matthews_corrcoef_v = round(matthews_corrcoef(new_y_test, predicted_label), DECIMAL_POINT)\n",
    "log_loss_v = round(log_loss(new_y_test, predicted_label), DECIMAL_POINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가지표 결과 저장 및 출력\n",
    "metric_total = pd.DataFrame({\n",
    "    \"PLM\" : \"bert\",\n",
    "    \"Optimizer\" : \"adam\",\n",
    "    \"Accuracy\" : accuracy_score_v,\n",
    "    \"Precision\" : precision_score_v,\n",
    "    \"Recall\" : recall_score_v,\n",
    "    \"F1_score\" : f1_score_v,\n",
    "    \"ROC_AUC_score\" : roc_auc_score_v,\n",
    "    \"Cohen_kappa_score\" : cohen_kappa_score_v,\n",
    "    \"Matthews_corrcoef\" : matthews_corrcoef_v,\n",
    "    \"Log_loss\" : log_loss_v\n",
    "    },\n",
    "    index = [\"-\"]\n",
    "    )\n",
    "metric_total.to_csv(OUT_METRIXFILE_PATH)\n",
    "print(metric_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sejong",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
