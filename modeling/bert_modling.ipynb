{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab 사용 시 주석 풀고 mount_path 설정 후 실행\n",
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab 사용시 주석 풀고 실행\n",
    "# !pip install transformers\n",
    "# vscode library 설치 안되있을 경우 실행\n",
    "# %pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab 사용시 주석 풀고 실행\n",
    "# !pip install seaborn\n",
    "# vscode library 설치 안되있을 경우 실행\n",
    "# %pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _init import *\n",
    "\n",
    "from commons import file_util, string_util\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# file_dir variable\n",
    "WORK_DIR = \"../../../\"\n",
    "IN_DIR = WORK_DIR + \"resources/keyword_extract/\"\n",
    "OUT_MODEL_PATH = WORK_DIR + \"resources/keyword_extract_model/bert_model.pt\"\n",
    "DELIM = \"\\t\"\n",
    "SETP_SIZE = 100\n",
    "\n",
    "# Hyperparameter variable\n",
    "BATCH_SIZE = 32\n",
    "MAX_SEQ_LEN = 260\n",
    "LEARNING_RATE = 2e-5\n",
    "EPOCHS = 25\n",
    "SEED_VAL = 0\n",
    "DROPOUT_RATE = 0.2\n",
    "DENCE_UNIT = 2\n",
    "\n",
    "# 사용할 bert model name 활성화\n",
    "# BERT_MODEL_NAME = \"bert-base-uncased\"\n",
    "# BERT_MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "BERT_MODEL_NAME = \"klue/bert-base\"\n",
    "# 이거는 너무 커서 안돌아감.\n",
    "# BERT_MODEL_NAME = \"klue/roberta-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# GPU divice name checker\n",
    "if device_name == \"/device:GPU:0\" :\n",
    "    print(\"Found GPU at : {}\".format(device_name))\n",
    "else :\n",
    "    print(\"GPU device not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device setting\n",
    "if torch.cuda.is_available() :\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print(\"There are %d GPU(s) available.\" % torch.cuda.device_count())\n",
    "    print(\"We will use the GPU : \", torch.cuda.get_device_name(0))\n",
    "else :\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU available, using the CPU instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vs code 사용 시 데이터 불러오기\n",
    "# data -> list로 넘길 때 사용\n",
    "def load(in_file_path: str, encoding: str, out_list: list) :\n",
    "\tfile_paths = file_util.get_file_paths(in_file_path, True)\n",
    "\n",
    "\tfor file_path in file_paths :\n",
    "\t\tin_file = file_util.open_file(file_path, encoding, \"r\")\n",
    "\n",
    "\t\twhile True :\n",
    "\t\t\tline = in_file.readline()\n",
    "\n",
    "\t\t\tif not line :\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tline = file_util.preprocess(line)\n",
    "\t\t\tif string_util.is_empty(line, True) :\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tout_list.append(line)\n",
    "\tin_file.close()\n",
    "\n",
    "# 이거 활성화 시켜서 사용하면 됨.\n",
    "# raw_data = []\n",
    "# load(IN_DIR, ENCODING, raw_data)\n",
    "# raw_data = [line.strip().split(DELIM) for line in raw_data]\n",
    "\n",
    "# raw_df = pd.DataFrame(raw_data, columns=[\"train\", \"label\"])\n",
    "# print(raw_df.shape)\n",
    "# raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name 추출\n",
    "file_paths = file_util.get_file_paths(IN_DIR, True)\n",
    "\n",
    "for file_path in file_paths :\n",
    "    file_name = file_util.get_file_name(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(IN_DIR + file_name, sep=DELIM, names=[\"train\", \"label\"])\n",
    "\n",
    "raw_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set, test_set 분할\n",
    "# 비율 조정 해서 train 8.1, validation 0.9, test 1.0\n",
    "print(\"여기서부터 문맥에 맞게 DATA_RATE 값 변경\")\n",
    "print(f\"train - test : {int(raw_df.shape[0] * 0.9)}\")\n",
    "DATA_RATE = 11277\n",
    "\n",
    "train_df = raw_df[:DATA_RATE]\n",
    "test_df = raw_df[DATA_RATE:]\n",
    "\n",
    "train_df.tail(), test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = train_df.train\n",
    "sentence[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_df.label.values\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME, do_lower_case=False)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentence]\n",
    "\n",
    "sentence[0], tokenized_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token embeding\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "\n",
    "# sentence를 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_SEQ_LEN, dtype=\"int64\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention masks initializer\n",
    "attention_masks = []\n",
    "\n",
    "for seq in input_ids :\n",
    "    seq_mask = [float(i > 0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "\n",
    "attention_masks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, validation 분리 -> train : 8, validation : 1, test : 1\n",
    "train_text, val_text, train_label, val_label = train_test_split(\n",
    "    input_ids,\n",
    "    labels,\n",
    "    random_state=0,\n",
    "    test_size=0.1\n",
    ")\n",
    "\n",
    "# attention_mask train, validation 분리\n",
    "train_masks, val_masks, _, _ = train_test_split(\n",
    "    attention_masks,\n",
    "    input_ids,\n",
    "    random_state=0,\n",
    "    test_size=0.1\n",
    ")\n",
    "\n",
    "# data to pytorch_tensor\n",
    "train_text = torch.tensor(train_text)\n",
    "train_label = torch.tensor(train_label)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "val_text = torch.tensor(val_text)\n",
    "val_label = torch.tensor(val_label)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "\n",
    "print(train_text[0])\n",
    "print(train_label[0])\n",
    "print(train_masks[0])\n",
    "print(val_text[0])\n",
    "print(val_label[0])\n",
    "print(val_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch learning\n",
    "train_data = TensorDataset(train_text, train_masks, train_label)\n",
    "train_sample = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(\n",
    "    train_data, \n",
    "    sampler=train_sample, \n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "validation_data = TensorDataset(val_text, val_masks, val_label)\n",
    "validation_sample = RandomSampler(validation_data)\n",
    "validation_dataloader = DataLoader(\n",
    "    validation_data, \n",
    "    sampler=validation_sample,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set setting\n",
    "sentence = test_df.train\n",
    "sentence[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = test_df.label.values\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME, do_lower_case=False)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentence]\n",
    "\n",
    "# sentence[0]\n",
    "tokenized_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "\n",
    "input_texts = pad_sequences(input_texts, maxlen=MAX_SEQ_LEN, dtype=\"int64\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "input_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = []\n",
    "\n",
    "for seq in input_texts :\n",
    "    seq_mask = [float(i > 0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "\n",
    "attention_masks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = torch.tensor(input_texts)\n",
    "test_labels = torch.tensor(labels)\n",
    "test_masks = torch.tensor(attention_masks)\n",
    "\n",
    "print(test_texts[0])\n",
    "print(test_labels[0])\n",
    "print(test_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TensorDataset(test_texts, test_masks, test_labels)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELS = len(train_df.label.unique())\n",
    "\n",
    "# 여기서 층마다 dropout, dence_unit, activation_function 설정가능 한지 확인\n",
    "model = BertForSequenceClassification.from_pretrained(BERT_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer setting\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr = LEARNING_RATE,     # learning_rate\n",
    "    eps = 1e-8              # 0으로 나누는 것을 방지하기 위한 epsilon_value\n",
    ")\n",
    "\n",
    "# train_step : batch count * epochs\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "\n",
    "schdeuler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flat_accuracy function\n",
    "def flat_accuracy(preds, labels) :\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time function\n",
    "def format_time(elapsed) :\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     56\u001b[0m \u001b[39m# BackWard\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     59\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), \u001b[39m1.0\u001b[39m)\n\u001b[0;32m     61\u001b[0m \u001b[39m# weigth update\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lklkk\\anaconda3\\envs\\sejong\\lib\\site-packages\\torch\\_tensor.py:255\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    247\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    248\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    249\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    253\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    254\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 255\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\lklkk\\anaconda3\\envs\\sejong\\lib\\site-packages\\torch\\autograd\\__init__.py:147\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m--> 147\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[0;32m    148\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    149\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 재현을 위해 랜덤시드 고정\n",
    "random.seed(SEED_VAL)\n",
    "np.random.seed(SEED_VAL)\n",
    "torch.manual_seed(SEED_VAL)\n",
    "torch.cuda.manual_seed_all(SEED_VAL)\n",
    "\n",
    "count = 0\n",
    "\n",
    "# gradant initializer\n",
    "model.zero_grad()\n",
    "\n",
    "# early_stopping 여기서 설정\n",
    "\n",
    "for epoch_i in range(EPOCHS) :\n",
    "    '''\n",
    "        Train\n",
    "    '''\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"======================== Epoch {:} / {:} =========================\".format(epoch_i + 1, EPOCHS))\n",
    "    print(\"Training...\")\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # loss initializer\n",
    "    total_loss = 0\n",
    "\n",
    "    # train mode change\n",
    "    model.train()\n",
    "\n",
    "    # dataloader from batch get data\n",
    "    for step, batch in enumerate(train_dataloader) :\n",
    "        if step % SETP_SIZE == 0 and not step == 0 :\n",
    "            elapsed = format_time(time.time() - start)\n",
    "            \n",
    "            print(\"\\tBatch {:} of {:}.\\t\\tElapsed: {:}.\".format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # GPU in batch\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # batch to data\n",
    "        b_input_texts, b_input_mask, b_labels = batch\n",
    "\n",
    "        # Forward 수행\n",
    "        outputs = model(\n",
    "            b_input_texts,\n",
    "            token_type_ids=None,\n",
    "            attention_mask=b_input_mask,\n",
    "            labels=b_labels\n",
    "        )\n",
    "\n",
    "        # loss calculator\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # BackWard\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # weigth update\n",
    "        optimizer.step()\n",
    "\n",
    "        # \n",
    "        schdeuler.step()\n",
    "\n",
    "        # gradent initalizer\n",
    "        model.zero_grad()\n",
    "\n",
    "    # avg loss\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"\\tAverage training loss : {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"\\tTraining epcoh took : {:}\".format(format_time(time.time() - start)))\n",
    "\n",
    "    '''\n",
    "        Validation\n",
    "    '''\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # model 평가\n",
    "    model.eval()\n",
    "\n",
    "    # value initalizer\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    for batch in validation_dataloader :\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        b_input_texts, b_input_mask, b_labels = batch\n",
    "\n",
    "        with torch.no_grad() :\n",
    "            outputs = model(\n",
    "                b_input_texts,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=b_input_mask,\n",
    "            )\n",
    "\n",
    "        logits = outputs[0]\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to(\"cpu\").numpy()\n",
    "\n",
    "        # output logits vs label accuracy\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(\"\\tAccuracy : {0:.2f}\".format(eval_accuracy / nb_eval_steps))\n",
    "    print(\"\\tValidation took : {:}\".format(format_time(time.time() - start)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set evalation\n",
    "start = time.time()\n",
    "\n",
    "# eval mode\n",
    "model.eval()\n",
    "\n",
    "# variable initalizer\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "# test dataloader batch_size loop\n",
    "for step, batch in enumerate(test_dataloader) :\n",
    "    if step % SETP_SIZE == 0 and not step == 0 :\n",
    "        elapsed = format_time(time.time() - start)\n",
    "        \n",
    "        print(\"\\tBatch {:} of {:}.\\tElapsed : {:}.\".format(step, len(test_dataloader), elapsed))\n",
    "\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    b_input_texts, b_input_mask, b_labels = batch\n",
    "\n",
    "    with torch.no_grad() :\n",
    "        outputs = model(\n",
    "            b_input_texts,\n",
    "            token_type_ids=None,\n",
    "            attention_mask=b_input_mask\n",
    "        )\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to(\"cpu\").numpy()\n",
    "\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "print(\"\")\n",
    "print(\"Accuracy : {0:.2f}\".format(eval_accuracy / nb_eval_steps))\n",
    "print(\"Test took : {:}\".format(format_time(time.time() - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 모델 저장\n",
    "torch.save(model.state_dict(), OUT_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장한 모델 불러오는 클래스\n",
    "class KeywordExtractorModel(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super(KeywordExtractorModel, self).__init__()\n",
    "        self.layer = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x) :\n",
    "        x = self.layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data convert\n",
    "def convert_input_data(sentence) :\n",
    "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentence]\n",
    "\n",
    "    # token embeding\n",
    "    input_texts = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "\n",
    "    input_texts = pad_sequences(\n",
    "        input_texts,\n",
    "        maxlen=MAX_SEQ_LEN, \n",
    "        dtype=\"long\", \n",
    "        truncating=\"post\", \n",
    "        padding=\"post\"\n",
    "    )\n",
    "    attention_masks = []\n",
    "\n",
    "    for seq in input_texts :\n",
    "        seq_mask = [float(i > 0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "    \n",
    "    inputs = torch.tensor(input_texts)\n",
    "    masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return inputs, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence teset\n",
    "def test_sentence(sentence) :\n",
    "    model.eval()\n",
    "\n",
    "    inputs, masks = convert_input_data(sentence)\n",
    "\n",
    "    b_input_texts = inputs.to(device)\n",
    "    b_input_masks = masks.to(device)\n",
    "\n",
    "    with torch.no_grad() :\n",
    "        outputs = model(\n",
    "            b_input_texts,\n",
    "            token_type_ids=None,\n",
    "            attention_mask=b_input_masks\n",
    "        )\n",
    "    \n",
    "    logits = outputs[0]\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test sentence 생성\n",
    "logits = test_sentence([\"연기는 별로지만 재미 하나는 끝내줌!\"])\n",
    "\n",
    "print(logits)\n",
    "print(np.argmax(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장한 model 불러오기 + 위의 문장 테스트 함수 이용해서 적용되는지 확인 가능.\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model = KeywordExtractorModel().to(device)\n",
    "\n",
    "# model_state_dict = torch.load(out_dir, map_location=device)\n",
    "# model.load_state_dict(model_state_dict)\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sejong",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
